name: "Observability Phase Implementation"
version: "1.0"

variables:
  observability_plan: "./OBSERVABILITY_PLAN.md"
  phase_number: "3"
  state_file: "./.observability-state.md"

steps:
  - id: "analyze"
    name: "Analyze Phase"
    prompt: |
      Read {{ observability_plan }} and analyze Phase {{ phase_number }}.

      Create a detailed implementation plan for this phase:
      - List all specific tasks
      - Identify files to modify or create
      - Define acceptance criteria
      - Estimate complexity

      Track your plan in {{ state_file }} under "Phase {{ phase_number }} Plan".

      End with: âœ… ANALYSIS_COMPLETE
    termination:
      success_pattern: "âœ… ANALYSIS_COMPLETE"
      on_success: "implement"

  - id: "implement"
    name: "Implement Phase"
    depends_on: ["analyze"]
    prompt: |
      Implement Phase {{ phase_number }} from the observability plan.

      Work from your analysis in {{ state_file }}. Complete all tasks to acceptance criteria.

      Track your progress in {{ state_file }} under "Phase {{ phase_number }} Implementation".

      Continue working until all tasks are complete.

      If you have more work to do in the next turn, end with: ðŸ”„ CONTINUE
      When fully complete, end with: âœ… IMPLEMENTATION_COMPLETE

      # Implementation Mode: TDD

      We want to follow Canonical TDD, Kent Beck style for this. For each component or set of functionality we come up with in the plan we will:
        1. Come up with a **test list** that describes the behaviour it should have. **Break big tasks** into atomic testable pieces. The list is livingâ€”add new tests as you discover edge cases, and **cross off tests** as you complete them. If there are existing tests that should change instead, add those to the list.
        2. Take the simplest test from the list, write it and **see it fail for the right reason** (e.g., method doesn't exist, wrong return valueâ€”not syntax error or unrelated failure). **ONE failing test** at a time per behavior.
        3. **Write test â†’ See it fail â†’ Minimal fix â†’ See it pass**. Implement just enough code to make the test pass, no more. When stuck, start with hard-coded values ("Fake It"), then use triangulation (add another test case) to force generalization to the real implementation.
        4. **Review the code and refactor** for readability, clarity and removing duplication. **Tests must stay green** during refactoring. Only refactor when all tests pass.
        5. **No skipping ahead**: Don't write production code except to pass a failing test. Don't write multiple tests at once.
        6. If there are more tests on the list, **go back to 2**.

        **Remember**: Red â†’ Green â†’ Refactor. Keep the cycle tight and disciplined.
    termination:
      success_pattern: "âœ… IMPLEMENTATION_COMPLETE"
      on_success: "review"
    continuation:
      pattern: "ðŸ”„ CONTINUE"
      action: "retry"

  - id: "review"
    name: "Review & Improve"
    depends_on: ["implement"]
    prompt: |
      Review the Phase {{ phase_number }} implementation.

      Check against the original plan in {{ observability_plan }} and your analysis in {{ state_file }}.

      Find opportunities for improvement. Document them in {{ state_file }} under "Phase {{ phase_number }} Review".

      Apply improvements if needed.

      If you have more work to do in the next turn, end with: ðŸ”„ CONTINUE

      If the review is complete and there's nothing else to do, commit all changes:
        git add -A
        git commit -m "[write a clear commit message describing what was implemented]"

      End with: âœ… REVIEW_COMPLETE
    termination:
      success_pattern: "âœ… REVIEW_COMPLETE"
      on_success: "smoke_test"
    continuation:
      pattern: "ðŸ”„ CONTINUE"
      action: "retry"

  - id: "smoke_test"
    name: "Smoke Test"
    depends_on: ["review"]
    prompt: |
      Run smoke tests to verify the implementation works correctly and has no regressions.

      Execute the following:
      1. Run: cargo test --lib
      2. Verify all tests pass
      3. Check for any warnings or errors in the output

      Things to verify:
      - All tests pass (currently ~76 tests)
      - No compilation errors
      - No test failures

      Note: There's a known issue with tests/smoke_test.rs being outdated. For now, we rely on lib tests for regression testing. You can optionally fix the smoke tests in tests/smoke_test.rs as a bonus task.

      If tests fail or there are issues, describe what's wrong and end with: ðŸ”„ CONTINUE

      If everything looks good, end with: âœ… SMOKE_TEST_PASSED
    termination:
      success_pattern: "âœ… SMOKE_TEST_PASSED"
    continuation:
      pattern: "ðŸ”„ CONTINUE"
      action: "retry"
